{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9ca683-fa17-404a-8fd0-6b9d441a5121",
   "metadata": {},
   "source": [
    "# SLB boxscore data scrape\n",
    "\n",
    "Extracts the data from each individual game and saves the boxscores in CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f5a50-1c51-44b3-94d4-72f446e5343f",
   "metadata": {},
   "source": [
    "First we need to load/import the various Python module we need to do this. I have added comments to give some indication of what the modules do.\n",
    "\n",
    "If you have not used selenium with Chrome before, you will probably need to download and install [Chrome for Testing](https://googlechromelabs.github.io/chrome-for-testing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4302533f-5673-4f9c-a406-40a964d77e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime # This allows us to manipulate dates\n",
    "from pathlib import Path # For checking if folders/directories exist and creating them\n",
    "import requests # For simple access to web pages\n",
    "import time # We're going to use this to pause the web browser for a couple of seconds\n",
    "import pandas as pd # For creating dataframes / CSV files\n",
    "import numpy as np # Numerical manipulations\n",
    "import re # For regular expressions - allows us to search with wildcards\n",
    "from bs4 import BeautifulSoup # For messing around with the data from web pages\n",
    "from selenium import webdriver # Automates using Chrome to access more complex web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30bcb08-121d-40a4-bac8-51a55b67f1fb",
   "metadata": {},
   "source": [
    "Lets grab the HTML from the SLB livestats page. We can use this to figure out where all of the boxscore information is kept. This will automatically open Chrome and should close it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175fa1f8-b050-4afb-b85a-b1204a578a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_url = \"https://www.superleaguebasketballm.co.uk/livestats/\"\n",
    "driver = webdriver.Chrome()\n",
    "page = driver.get(results_url)\n",
    "driver.switch_to.frame(1)\n",
    "results_soup = BeautifulSoup(driver.page_source, 'html')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d945af3-f01c-40b0-bf71-fdbdd3de7e43",
   "metadata": {},
   "source": [
    "Now we want to extract all of the URLs for the games that have been played (it skips over any that currently have the score listed as 'Upcoming').\n",
    "\n",
    "If we wanted to extract only the data for a specific competition, then we can uncomment the lines that include `comp_string'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b894b61d-c6bb-4d40-88aa-2eb1cc45c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = results_soup.find_all('table')\n",
    "#comp_string = 'Championship 2024-25'\n",
    "upcoming_string = 'Upcoming'\n",
    "url_soup = []\n",
    "for rows in tables[0].find_all('tr'):\n",
    "    cells = rows.find_all('td')\n",
    "#    if comp_string in cells[2]:\n",
    "    for a in rows.find_all('a', href=True):\n",
    "        if a.contents[0] != upcoming_string:\n",
    "            url_soup.append(a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5334c-c961-4906-87a0-abea13fb6a4f",
   "metadata": {},
   "source": [
    "We can use the SLB URLs gethered above to directly access the FIBA live stats. This next cell makes a list of the live stats URLs for all of the games identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fe4ac1-a55d-4359-8aed-0122462b0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = []\n",
    "for url in url_soup:\n",
    "    game_id.append(url.split('/')[5])\n",
    "\n",
    "league = 'SLB'\n",
    "baseurl = 'https://www.fibalivestats.com/u/{}'.format(league)\n",
    "games = []\n",
    "\n",
    "for g_id in game_id:\n",
    "    url = \"{}/{}/\".format(baseurl, g_id)\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        #print(url)\n",
    "        games.append(url)\n",
    "    else:\n",
    "        print(\"Couldn't resolve URL:\", url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe2fc1-3f28-49b7-812b-74fcd3f2238c",
   "metadata": {},
   "source": [
    "We now define some functions that work with the live stats URLs to scrape all of the boxscore data and save the data as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8140f3-f13d-42d2-9364-22a4372a27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_to_df(soup):\n",
    "    \"\"\"Converts the soup of FIBA livestats for a single game into a data frame. \n",
    "    The data frame, the teams playing and the date the game was played are then returned\"\"\"\n",
    "    teams=[]\n",
    "    team_divs = soup.find_all(\"div\", {\"class\": \"team-name\"})\n",
    "    for count, div in enumerate(team_divs):\n",
    "        team_span = div.find_all('span')\n",
    "        teams.append(team_span[0].get_text())\n",
    "    \n",
    "    date=soup.find_all(\"div\", {\"class\": \"og-date\"})[0].get_text()\n",
    "    date_formatted = datetime.strptime(date, '%d/%m/%Y')\n",
    "    date = date_formatted.strftime('%Y%m%d')\n",
    "\n",
    "    #Create the internal lists to hold all of the data\n",
    "    player_name=[]\n",
    "    team=[]\n",
    "    minutes=[]\n",
    "    points=[]\n",
    "    fgm=[]\n",
    "    fga=[]\n",
    "    fgper=[]\n",
    "    twopm=[]\n",
    "    twopa=[]\n",
    "    twoper=[]\n",
    "    threepm=[]\n",
    "    threepa=[]\n",
    "    threeper=[]\n",
    "    ftpm=[]\n",
    "    ftpa=[]\n",
    "    ftper=[]\n",
    "    rebo=[]\n",
    "    rebd=[]\n",
    "    rebtot=[]\n",
    "    assists=[]\n",
    "    tos=[]\n",
    "    steals=[]\n",
    "    blocks=[]\n",
    "    blocksr=[]\n",
    "    fouls=[]\n",
    "    foulson=[]\n",
    "    plusminus=[]\n",
    "\n",
    "    #Populate the lists\n",
    "    scores_tables = soup.find_all(\"table\", {\"class\": \"boxscore\"})\n",
    "    for team_count, table in enumerate(scores_tables):\n",
    "        for count, row in enumerate(table.find_all('tr', {\"class\": \"player-row\"})):\n",
    "            if count != 0:\n",
    "                player_name.append(row.find_all('a', {\"class\": \"playerpopup\"})[0].find_all('span')[0].get_text())\n",
    "                team.append(teams[team_count])\n",
    "                minutes.append(row.find_all('span', {\"id\": re.compile(\"Minutes\")})[0].get_text())\n",
    "                points.append(row.find_all('span', {\"id\": re.compile(\"Points\")})[0].get_text())\n",
    "                fgm.append(row.find_all('span', {\"id\": re.compile(\"FieldGoalsMade\")})[0].get_text())\n",
    "                fga.append(row.find_all('span', {\"id\": re.compile(\"FieldGoalsAttempted\")})[0].get_text())\n",
    "                fgper.append(row.find_all('span', {\"id\": re.compile(\"FieldGoalsPercentage\")})[0].get_text())\n",
    "                twopm.append(row.find_all('span', {\"id\": re.compile(\"TwoPointersMade\")})[0].get_text())\n",
    "                twopa.append(row.find_all('span', {\"id\": re.compile(\"TwoPointersAttempted\")})[0].get_text())\n",
    "                twoper.append(row.find_all('span', {\"id\": re.compile(\"TwoPointersPercentage\")})[0].get_text())\n",
    "                threepm.append(row.find_all('span', {\"id\": re.compile(\"ThreePointersMade\")})[0].get_text())\n",
    "                threepa.append(row.find_all('span', {\"id\": re.compile(\"ThreePointersAttempted\")})[0].get_text())\n",
    "                threeper.append(row.find_all('span', {\"id\": re.compile(\"ThreePointersPercentage\")})[0].get_text())\n",
    "                ftpm.append(row.find_all('span', {\"id\": re.compile(\"FreeThrowsMade\")})[0].get_text())\n",
    "                ftpa.append(row.find_all('span', {\"id\": re.compile(\"FreeThrowsAttempted\")})[0].get_text())\n",
    "                ftper.append(row.find_all('span', {\"id\": re.compile(\"FreeThrowsPercentage\")})[0].get_text())\n",
    "                rebo.append(row.find_all('span', {\"id\": re.compile(\"ReboundsOffensive\")})[0].get_text())\n",
    "                rebd.append(row.find_all('span', {\"id\": re.compile(\"ReboundsDefensive\")})[0].get_text())\n",
    "                rebtot.append(row.find_all('span', {\"id\": re.compile(\"ReboundsTotal\")})[0].get_text())\n",
    "                assists.append(row.find_all('span', {\"id\": re.compile(\"Assists\")})[0].get_text())\n",
    "                tos.append(row.find_all('span', {\"id\": re.compile(\"Turnovers\")})[0].get_text())\n",
    "                steals.append(row.find_all('span', {\"id\": re.compile(\"Steals\")})[0].get_text())\n",
    "                blocks.append(row.find_all('span', {\"id\": re.compile(\"Blocks\")})[0].get_text())\n",
    "                blocksr.append(row.find_all('span', {\"id\": re.compile(\"BlocksReceived\")})[0].get_text())\n",
    "                fouls.append(row.find_all('span', {\"id\": re.compile(\"FoulsPersonal\")})[0].get_text())\n",
    "                foulson.append(row.find_all('span', {\"id\": re.compile(\"FoulsOn\")})[0].get_text())\n",
    "                plusminus.append(row.find_all('span', {\"id\": re.compile(\"PlusMinusPoints\")})[0].get_text())\n",
    "\n",
    "    #Create the dataframe\n",
    "    df = pd.DataFrame(np.column_stack([player_name, team, minutes, points, fgm, fga, fgper, twopm, twopa, twoper, threepm, threepa, threeper, \n",
    "                                   ftpm, ftpa, ftper, rebo, rebd, rebtot, assists, tos, steals, blocks, blocksr, fouls, foulson, plusminus]), \n",
    "                                   columns=[\"Name\", \"Team\", \"Mins\", \"PTS\", \"FGM\", \"FGA\", \"FG%\", \"2PM\", \"2PA\", \"2P%\", \"3PM\", \"3PA\", \"3P%\", \n",
    "                                    \"FTM\", \"FTA\", \"FT%\",\"OREB\", \"DREB\", \"REB\", \"AST\", \"TO\", \"STL\", \"BLK\", \"BLKR\", \"PF\", \n",
    "                                    \"FOULON\", \"PLUSMINUS\"])\n",
    "    \n",
    "    return df, teams, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9425c834-48e5-4e1d-93b5-ac87bd494e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiba_url_to_soup(game):\n",
    "    \"\"\"Takes the base FIBA livestats URL, adds the extra info to request the boxscore, then returns the pagesoup\"\"\"\n",
    "    url = game+'bs.html'\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html')\n",
    "    browser.close()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2780fe16-a152-4777-8287-15db8878302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_game_csv(df, teams, date, directory=None):\n",
    "    \"\"\"Saves the dataframe in CSV format, with the filename generated from the teams and date\n",
    "    Optionally places the file into a directory\"\"\"\n",
    "    filename = teams[0].replace(\" \", \"-\") + \"-Vs-\" + teams[1].replace(\" \", \"-\") + \"-\" + date + \".csv\"\n",
    "    if directory == None:\n",
    "        df.to_csv(filename)\n",
    "    else:\n",
    "        if not Path(directory).is_dir():\n",
    "            Path(directory).mkdir()\n",
    "        df.to_csv(Path(directory, filename))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d1157-1c22-41a5-86d5-7fa55f6d0e66",
   "metadata": {},
   "source": [
    "The next cell does the work of scraping the data and saving to CSV for each game identified above. Please note that it will launch a Chrome window (and eventually close it) for every game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ef14d2-4bcd-4754-abf4-eee821f7bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for game in games:\n",
    "    soup = fiba_url_to_soup(game)\n",
    "    df, teams, date = stats_to_df(soup)\n",
    "    save_game_csv(df, teams, date, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda9a31e-ede0-43a2-a3f8-65493a4e7ebb",
   "metadata": {},
   "source": [
    "We should now have a separate CSV for each game that has been played, all in the directory called data. Processing and analysing the data will take place in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcd4c4-17ff-41e2-97d7-e410e617992b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
